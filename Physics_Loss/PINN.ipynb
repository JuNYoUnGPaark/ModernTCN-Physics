{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c37b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "  - Parameters: 75,163\n",
      "[Physics] Epoch 025/150 | Train Acc=97.27% | CE=0.0632 | Phys=0.1826 (Î»=0.1) | Total=0.0815 || Test F1=0.9583 | Test Acc=95.79% (Best F1=0.9607, Best Acc=96.03%)\n",
      "[Physics] Epoch 050/150 | Train Acc=99.47% | CE=0.0167 | Phys=0.1277 (Î»=0.1) | Total=0.0294 || Test F1=0.9716 | Test Acc=97.18% (Best F1=0.9746, Best Acc=97.46%)\n",
      "[Physics] Epoch 075/150 | Train Acc=99.67% | CE=0.0096 | Phys=0.0987 (Î»=0.1) | Total=0.0194 || Test F1=0.9742 | Test Acc=97.39% (Best F1=0.9783, Best Acc=97.83%)\n",
      "[Physics] Epoch 100/150 | Train Acc=99.77% | CE=0.0061 | Phys=0.0845 (Î»=0.1) | Total=0.0146 || Test F1=0.9735 | Test Acc=97.32% (Best F1=0.9783, Best Acc=97.83%)\n",
      "[Physics] Epoch 125/150 | Train Acc=99.78% | CE=0.0052 | Phys=0.0779 (Î»=0.1) | Total=0.0129 || Test F1=0.9704 | Test Acc=97.01% (Best F1=0.9783, Best Acc=97.83%)\n",
      "[Physics] Epoch 150/150 | Train Acc=99.92% | CE=0.0022 | Phys=0.0755 (Î»=0.1) | Total=0.0098 || Test F1=0.9704 | Test Acc=97.01% (Best F1=0.9783, Best Acc=97.83%)\n",
      "Model                               |              Best Test F1 |         Best Test Acc (%)\n",
      "-----------------------------------------------------------------------------------------\n",
      "Full Model                          |                    0.9783 |                     97.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt    \n",
    "from sklearn.metrics import f1_score    \n",
    "\n",
    "# ========================\n",
    "# SEED ê³ ì • í•¨ìˆ˜\n",
    "# ========================\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # CUDA 11+\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=False)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ========================\n",
    "# UCI-HAR ë°ì´í„° ë¡œë“œ \n",
    "# ========================\n",
    "class UCIHARDataset(Dataset):\n",
    "    def __init__(self, data_path, split='train'):\n",
    "        base = Path(data_path) / split\n",
    "        signals = []\n",
    "        for sensor in ['body_acc', 'body_gyro', 'total_acc']:\n",
    "            for axis in ['x', 'y', 'z']:\n",
    "                file = base / 'Inertial Signals' / f'{sensor}_{axis}_{split}.txt'\n",
    "                signals.append(np.loadtxt(file))\n",
    "\n",
    "        self.X = np.stack(signals, axis=-1)\n",
    "        self.y = np.loadtxt(base.parent / split / f'y_{split}.txt').astype(int) - 1\n",
    "\n",
    "        try:\n",
    "            self.subjects = np.loadtxt(base.parent / split / f'subject_{split}.txt').astype(int)\n",
    "        except:\n",
    "            self.subjects = np.ones(len(self.y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.FloatTensor(self.X[idx]),\n",
    "                torch.LongTensor([self.y[idx]])[0],\n",
    "                self.subjects[idx])\n",
    "\n",
    "# ========================\n",
    "# ğŸ”¥ Modern TCN Components \n",
    "# ========================\n",
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(\n",
    "            in_channels, in_channels, kernel_size,\n",
    "            padding=padding, dilation=dilation, groups=in_channels\n",
    "        )\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class MultiScaleConvBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_sizes=[3, 5, 7], dilation=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.branches = nn.ModuleList()\n",
    "        for k in kernel_sizes:\n",
    "            padding = ((k - 1) * dilation) // 2\n",
    "            branch = nn.ModuleDict({\n",
    "                'conv': DepthwiseSeparableConv1d(channels, channels, k, dilation, padding),\n",
    "                'norm': nn.BatchNorm1d(channels),\n",
    "                'dropout': nn.Dropout(dropout)\n",
    "            })\n",
    "            self.branches.append(branch)\n",
    "        self.fusion = nn.Conv1d(channels * len(kernel_sizes), channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        target_length = x.size(2)\n",
    "        for branch in self.branches:\n",
    "            out = branch['conv'](x)\n",
    "            if out.size(2) != target_length:\n",
    "                out = out[:, :, :target_length]\n",
    "            out = branch['norm'](out)\n",
    "            out = F.gelu(out)\n",
    "            out = branch['dropout'](out)\n",
    "            outputs.append(out)\n",
    "        multi_scale = torch.cat(outputs, dim=1)\n",
    "        return self.fusion(multi_scale)\n",
    "\n",
    "class ModernTCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes=[3, 7], dilation=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # NOTE: kernel_sizesê°€ [7]ì²˜ëŸ¼ ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë“¤ì–´ì˜¤ë©´ Single-scaleì´ ë¨\n",
    "        self.multi_conv1 = MultiScaleConvBlock(\n",
    "            in_channels if in_channels == out_channels else out_channels,\n",
    "            kernel_sizes, dilation, dropout\n",
    "        )\n",
    "        \n",
    "        # NOTE: kernel_sizes ì¤‘ ê°€ì¥ í° ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ padding\n",
    "        max_k = max(kernel_sizes) if isinstance(kernel_sizes, list) else kernel_sizes\n",
    "        padding = ((max_k - 1) * dilation) // 2\n",
    "        \n",
    "        self.conv2 = DepthwiseSeparableConv1d(\n",
    "            out_channels, out_channels, max_k, dilation, padding\n",
    "        )\n",
    "        self.norm2 = nn.BatchNorm1d(out_channels)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        target_length = x.size(2)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "            residual = x\n",
    "        \n",
    "        out = self.multi_conv1(x)\n",
    "        if out.size(2) != target_length:\n",
    "            out = out[:, :, :target_length]\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        if out.size(2) != target_length:\n",
    "            out = out[:, :, :target_length]\n",
    "        out = self.norm2(out)\n",
    "        out = F.gelu(out)\n",
    "        out = self.dropout2(out)\n",
    "        return F.gelu(out + residual)\n",
    "\n",
    "class SqueezeExcitation1d(nn.Module):\n",
    "    def __init__(self, channels, reduction=5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "    def forward(self, x):\n",
    "        batch, channels, _ = x.size()\n",
    "        squeeze = F.adaptive_avg_pool1d(x, 1).view(batch, channels)\n",
    "        excitation = F.relu(self.fc1(squeeze))\n",
    "        excitation = torch.sigmoid(self.fc2(excitation)).view(batch, channels, 1)\n",
    "        return x * excitation\n",
    "\n",
    "class LargeKernelConv1d(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=21):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.depthwise = nn.Conv1d(\n",
    "            channels, channels, kernel_size,\n",
    "            padding=padding, groups=channels\n",
    "        )\n",
    "        self.norm = nn.BatchNorm1d(channels)\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "\n",
    "# ========================\n",
    "# Modern TCN Base ëª¨ë¸ \n",
    "# ========================\n",
    "class BaseModernTCNHAR(nn.Module):\n",
    "    def __init__(self, input_dim=9, hidden_dim=128, n_layers=4, n_classes=6,\n",
    "                 kernel_sizes=[3, 7], large_kernel=21, dropout=0.1, use_se=True):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Conv1d(input_dim, hidden_dim, 1)\n",
    "        self.large_kernel_conv = LargeKernelConv1d(hidden_dim, large_kernel)\n",
    "        self.tcn_blocks = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            dilation = 2 ** i\n",
    "            self.tcn_blocks.append(\n",
    "                ModernTCNBlock(\n",
    "                    hidden_dim, hidden_dim,\n",
    "                    kernel_sizes=kernel_sizes,\n",
    "                    dilation=dilation,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "            )\n",
    "        self.final_large_kernel = LargeKernelConv1d(hidden_dim, large_kernel)\n",
    "        # self.final_large_kernel = nn.Identity()\n",
    "        self.use_se = use_se\n",
    "        if use_se:\n",
    "            self.se = SqueezeExcitation1d(hidden_dim)\n",
    "        self.norm_final = nn.LayerNorm(hidden_dim)\n",
    "        self.head = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.large_kernel_conv(x)\n",
    "        x = F.gelu(x)\n",
    "        for block in self.tcn_blocks:\n",
    "            x = block(x)\n",
    "        x = self.final_large_kernel(x)\n",
    "        x = F.gelu(x)\n",
    "        if self.use_se:\n",
    "            x = self.se(x)\n",
    "        x = F.adaptive_avg_pool1d(x, 1).squeeze(-1)\n",
    "        x = self.norm_final(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# ========================\n",
    "# Physics-Guided Modern TCN HAR \n",
    "# ========================\n",
    "class PhysicsModernTCNHAR(BaseModernTCNHAR):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        hidden_dim = self.head.in_features\n",
    "        self.physics_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 9)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_physics=False):\n",
    "        x = x.transpose(1, 2)\n",
    "        x_feat = self.input_proj(x)\n",
    "        x_feat = self.large_kernel_conv(x_feat)\n",
    "        x_feat = F.gelu(x_feat)\n",
    "        for block in self.tcn_blocks:\n",
    "            x_feat = block(x_feat)\n",
    "        x_feat = self.final_large_kernel(x_feat)\n",
    "        x_feat = F.gelu(x_feat)\n",
    "        if self.use_se:\n",
    "            x_feat = self.se(x_feat)\n",
    "        \n",
    "        # 1. ë¶„ë¥˜ í—¤ë“œ\n",
    "        pooled = F.adaptive_avg_pool1d(x_feat, 1).squeeze(-1)\n",
    "        pooled = self.norm_final(pooled)\n",
    "        logits = self.head(pooled)\n",
    "\n",
    "        if return_physics:\n",
    "            # 2. ë¬¼ë¦¬ í—¤ë“œ\n",
    "            x_feat_transposed = x_feat.transpose(1, 2)\n",
    "            physics = self.physics_head(x_feat_transposed)\n",
    "            return logits, physics\n",
    "\n",
    "        return logits\n",
    "\n",
    "# ========================\n",
    "# 'ë¬¼ë¦¬ ì†ì‹¤' í•¨ìˆ˜ \n",
    "# ========================\n",
    "def physics_loss_pinn_v1(physics_pred, X_raw, lambda_residual=1.0):\n",
    "    \"\"\"\n",
    "    PINN ì² í•™ì„ ì ìš©í•œ ë¬¼ë¦¬ ì†ì‹¤ í•¨ìˆ˜.\n",
    "    \n",
    "    ë¬¼ë¦¬ ë²•ì¹™: total_acc = body_acc + gravity\n",
    "    ì”ì°¨ (f): (pred_body_acc + pred_gravity) - true_total_acc\n",
    "    \n",
    "    L_total = L_recon (ë°ì´í„° ì†ì‹¤) + lambda_residual * L_phys_residual (ë¬¼ë¦¬ ì†ì‹¤)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. ì˜ˆì¸¡ê°’ ë¶„ë¦¬ (ë¬¼ë¦¬ í—¤ë“œ ì¶œë ¥ = 9ì±„ë„) ---\n",
    "    # (B, T, 9) -> (B, T, 3) 3ê°œ\n",
    "    pred_body_acc = physics_pred[:, :, 0:3]\n",
    "    pred_gyro     = physics_pred[:, :, 3:6]\n",
    "    pred_gravity  = physics_pred[:, :, 6:9] # ì˜ˆì¸¡ëœ ì¤‘ë ¥ (Latent)\n",
    "\n",
    "    # --- 2. ì‹¤ì œê°’ ë¶„ë¦¬ (ì›ë³¸ X = 9ì±„ë„) ---\n",
    "    true_body_acc = X_raw[:, :, 0:3]\n",
    "    true_gyro     = X_raw[:, :, 3:6]\n",
    "    true_total_acc = X_raw[:, :, 6:9] # ì‹¤ì œ ì´ ê°€ì†ë„\n",
    "\n",
    "    # --- 3. ì†ì‹¤ ê³„ì‚° ---\n",
    "    \n",
    "    # [A] L_recon (ë°ì´í„° ì†ì‹¤): ê¸°ì¡´ì˜ ë³µì› ì†ì‹¤ (body_acc, gyroë§Œ)\n",
    "    loss_recon = F.smooth_l1_loss(pred_body_acc, true_body_acc) + \\\n",
    "                 F.smooth_l1_loss(pred_gyro, true_gyro)\n",
    "\n",
    "    # [B] L_phys_residual (ë¬¼ë¦¬ ì†ì‹¤): PINNì˜ ì”ì°¨ ì†ì‹¤\n",
    "    \n",
    "    # ë¶€í’ˆìœ¼ë¡œ ì˜ˆì¸¡í•œ ì´ ê°€ì†ë„\n",
    "    pred_total_from_parts = pred_body_acc + pred_gravity \n",
    "    \n",
    "    # ì”ì°¨: (ì˜ˆì¸¡ ë¶€í’ˆ í•©)ê³¼ (ì‹¤ì œ ì´ ê°€ì†ë„)ê°€ ì¼ì¹˜í•´ì•¼ í•¨\n",
    "    loss_phys_residual = F.smooth_l1_loss(pred_total_from_parts, true_total_acc)\n",
    "\n",
    "    # --- 4. ìµœì¢… ì†ì‹¤ ì¡°í•© ---\n",
    "    # L_recon (2ê°œ í•­ì˜ í•©)ê³¼ L_phys_residual (1ê°œ í•­)ì„ ê°€ì¤‘ì¹˜(lambda)ë¡œ ì¡°ì ˆ\n",
    "    return loss_recon + lambda_residual * loss_phys_residual\n",
    "\n",
    "# ========================\n",
    "# í—¬í¼: íŒŒë¼ë¯¸í„° ì¹´ìš´íŠ¸\n",
    "# ========================\n",
    "def get_n_params(model):\n",
    "    return f\"{sum(p.numel() for p in model.parameters() if p.requires_grad):,}\"\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 'ë¬¼ë¦¬ ê¸°ë°˜' í•™ìŠµ í•¨ìˆ˜ (F1 Score ê¸°ì¤€)\n",
    "# ========================\n",
    "def train_physics(model, train_loader, test_loader, device, \n",
    "                  epochs=50, lambda_phys=0.05, log_every=1,\n",
    "                  lambda_phys_residual=1.0,):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "    best_f1, best_acc = 0.0, 0.0\n",
    "    # ì†ì‹¤ íˆìŠ¤í† ë¦¬ ì €ì¥ (ì—í­ ë‹¨ìœ„)\n",
    "    history = {\n",
    "        \"train_ce\": [], \"train_phys\": [], \"train_total\": [],\n",
    "        \"test_acc\": [], \"test_f1\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        # ì—í­ ëˆ„ì ìš©\n",
    "        ce_sum, phys_sum, total_sum = 0.0, 0.0, 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for X, y, _ in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, physics = model(X, return_physics=True)\n",
    "            loss_ce = F.cross_entropy(logits, y)\n",
    "            loss_phys = physics_loss_pinn_v1(physics, X, lambda_phys_residual)\n",
    "\n",
    "            loss = loss_ce + lambda_phys * loss_phys\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # ëˆ„ì  (ì£¼ì˜: .item()ìœ¼ë¡œ ìˆ«ìí™”)\n",
    "            ce_sum   += loss_ce.item()\n",
    "            phys_sum += loss_phys.item()\n",
    "            total_sum += loss.item()\n",
    "\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total   += y.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # ì—í­ í‰ê· \n",
    "        n_batches = len(train_loader)\n",
    "        ce_avg   = ce_sum / n_batches\n",
    "        phys_avg = phys_sum / n_batches\n",
    "        total_avg = total_sum / n_batches\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # ====== í‰ê°€ ======\n",
    "        model.eval()\n",
    "        test_correct, test_total = 0, 0\n",
    "        all_preds, all_y = [], []\n",
    "        with torch.inference_mode():\n",
    "            for X, y, _ in test_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                logits = model(X, return_physics=False)\n",
    "                preds = logits.argmax(1)\n",
    "                test_correct += (preds == y).sum().item()\n",
    "                test_total   += y.size(0)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_y.extend(y.cpu().numpy())\n",
    "\n",
    "        test_acc = 100.0 * test_correct / test_total\n",
    "        test_f1 = f1_score(all_y, all_preds, labels=list(range(6)),\n",
    "                           average='macro', zero_division=0)\n",
    "        best_f1 = max(best_f1, test_f1)\n",
    "        best_acc = max(best_acc, test_acc)\n",
    "\n",
    "        # ë¡œê·¸ & íˆìŠ¤í† ë¦¬\n",
    "        history[\"train_ce\"].append(ce_avg)\n",
    "        history[\"train_phys\"].append(phys_avg)\n",
    "        history[\"train_total\"].append(total_avg)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "        history[\"test_f1\"].append(test_f1)\n",
    "\n",
    "        if (epoch % log_every) == 0:\n",
    "            print(f\"[Physics] Epoch {epoch:03d}/{epochs} | \"\n",
    "                  f\"Train Acc={train_acc:.2f}% | \"\n",
    "                  f\"CE={ce_avg:.4f} | Phys={phys_avg:.4f} (Î»={lambda_phys}) | \"\n",
    "                  f\"Total={total_avg:.4f} || \"\n",
    "                  f\"Test F1={test_f1:.4f} | Test Acc={test_acc:.2f}% \"\n",
    "                  f\"(Best F1={best_f1:.4f}, Best Acc={best_acc:.2f}%)\")\n",
    "\n",
    "    return best_f1, best_acc, history\n",
    "\n",
    "\n",
    "# ========================\n",
    "# ì‹œê°í™” í•¨ìˆ˜\n",
    "# ========================\n",
    "def plot_results(results_dict):\n",
    "    print(f\"{'Model':<35} | {'Best Test F1':>25} | {'Best Test Acc (%)':>25}\")\n",
    "    print(\"-\" * 89) \n",
    "    \n",
    "    # ë”•ì…”ë„ˆë¦¬ì—ì„œ F1ê³¼ Accë¥¼ ëª¨ë‘ êº¼ë‚´ì„œ ì¶œë ¥\n",
    "    for name, metrics in results_dict.items():\n",
    "        print(f\"{name:<35} | {metrics['f1']:>25.4f} | {metrics['acc']:>25.2f}\")\n",
    "    print(\"-\" * 89)\n",
    "    print(\"=\"*90)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "# ========================\n",
    "def main():\n",
    "    set_seed(42) \n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    data_path = 'C://Users/park9/ModernTCN_Physics/data'\n",
    "\n",
    "    try:\n",
    "        train_ds = UCIHARDataset(data_path, split='train')\n",
    "        test_ds = UCIHARDataset(data_path, split='test')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'data_path' ë³€ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.\")\n",
    "        print(f\"í˜„ì¬ ê²½ë¡œ: {data_path}\")\n",
    "        return\n",
    "\n",
    "    def compute_train_stats(ds):\n",
    "        # ds.X: numpy (N, T, C)\n",
    "        X = torch.from_numpy(ds.X)             # (N, T, 9) float64ì¼ ìˆ˜ ìˆìŒ\n",
    "        X = X.float()\n",
    "        mean = X.mean(dim=(0,1), keepdim=True) # (1,1,C)\n",
    "        std  = X.std(dim=(0,1), keepdim=True).clamp_min(1e-6)\n",
    "        return mean, std\n",
    "\n",
    "    mean, std = compute_train_stats(train_ds)\n",
    "\n",
    "    def make_loaders(train_ds, test_ds, mean, std, batch_size=64, seed=42):\n",
    "        def collate_norm(batch):\n",
    "            X, y, s = zip(*batch)\n",
    "            X = torch.stack(X, dim=0).float()\n",
    "            X = (X - mean) / std\n",
    "            y = torch.tensor(y, dtype=torch.long)\n",
    "            s = torch.tensor(s, dtype=torch.long)\n",
    "            return X, y, s\n",
    "\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_ds, batch_size=batch_size, shuffle=True,\n",
    "            generator=g, num_workers=0, drop_last=True,\n",
    "            pin_memory=torch.cuda.is_available(), collate_fn=collate_norm\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_ds, batch_size=batch_size, shuffle=False,\n",
    "            num_workers=0, drop_last=False,\n",
    "            pin_memory=torch.cuda.is_available(), collate_fn=collate_norm\n",
    "        )\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    EPOCHS = 150 \n",
    "    results = {} \n",
    "    \n",
    "    # ---\n",
    "    # Full Model\n",
    "    # ---\n",
    "    set_seed(42)\n",
    "    train_loader, test_loader = make_loaders(train_ds, test_ds, mean, std, seed=42)\n",
    "    model3 = PhysicsModernTCNHAR( \n",
    "        input_dim=9, hidden_dim=64, n_layers=3, \n",
    "        n_classes=6,\n",
    "        kernel_sizes=[3, 7], \n",
    "        large_kernel=19, \n",
    "        dropout=0.1,\n",
    "        use_se=True        \n",
    "    ).to(device)\n",
    "    print(f\"  - Parameters: {get_n_params(model3)}\")\n",
    "    f1, acc, hist = train_physics(model3, train_loader, test_loader, device,\n",
    "                                      epochs=EPOCHS, lambda_phys=0.1, log_every=25,\n",
    "                                      lambda_phys_residual=2.5,) \n",
    "    results['Full Model'] = {'f1': f1, 'acc': acc}\n",
    "\n",
    "    plot_results(results)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (har-cu126)",
   "language": "python",
   "name": "har-cu126"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
